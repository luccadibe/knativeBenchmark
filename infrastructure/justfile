# Required environment variables (set these before running commands)
export TF_VAR_project_id := "csb-benchmark"       # Your GCP project ID
export CLUSTER_NAME := "csb.lucca.org"            # Your cluster name
export TF_VAR_zone := "europe-west1-b"             # GCP zone for the cluster
export NODE_COUNT := "5"              # 1 monitoring + 1 workload-gen + 1 knative-system + 2 for functions
export CONTROL_PLANE_SIZE := "e2-standard-2"      # Machine type for control plane
export NODE_SIZE := "e2-standard-2"               # Machine type for worker nodes
export TF_VAR_bucket_name := "csb-benchmark-state"      # Must be globally unique
export TF_VAR_bucket_location := "EU"  # GCS bucket location

# Required environment variables check
_check-env:
    #!/usr/bin/env bash
    missing_vars=()
    [[ -z "${TF_VAR_project_id}" ]] && missing_vars+=("TF_VAR_project_id")
    [[ -z "${CLUSTER_NAME}" ]] && missing_vars+=("CLUSTER_NAME")
    [[ -z "${TF_VAR_zone}" ]] && missing_vars+=("TF_VAR_zone")
    [[ -z "${NODE_COUNT}" ]] && missing_vars+=("NODE_COUNT")
    [[ -z "${CONTROL_PLANE_SIZE}" ]] && missing_vars+=("CONTROL_PLANE_SIZE")
    [[ -z "${NODE_SIZE}" ]] && missing_vars+=("NODE_SIZE")
    [[ -z "${TF_VAR_bucket_name}" ]] && missing_vars+=("TF_VAR_bucket_name")
    [[ -z "${TF_VAR_bucket_location}" ]] && missing_vars+=("TF_VAR_bucket_location")
    
    if [ ${#missing_vars[@]} -ne 0 ]; then
        echo "Error: Missing required environment variables:"
        printf '%s\n' "${missing_vars[@]}"
        echo "Please set the required variables at the top of the justfile"
        exit 1
    fi

# List available recipes
default:
    @just --list

# Setup required GCP APIs
setup: _check-env
    #!/bin/bash
    gcloud services enable cloudresourcemanager.googleapis.com
    gcloud services enable compute.googleapis.com
    gcloud services enable iam.googleapis.com
    gcloud services enable container.googleapis.com
    gcloud services enable storage.googleapis.com

# Initialize terraform
init: _check-env
    terraform init

# Create and configure the cluster
up spot="": _check-env
    #!/bin/bash
    set -e
    
    # Create GCS bucket
    terraform apply -auto-approve

    # Set kOps state store
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"

    # Create cluster configuration
    echo "Creating cluster configuration..."
    kops create cluster \
        --name="${CLUSTER_NAME}" \
        --state="${KOPS_STATE_STORE}" \
        --zones="${TF_VAR_zone}" \
        --control-plane-zones="${TF_VAR_zone}" \
        --node-count="${NODE_COUNT}" \
        --control-plane-size="${CONTROL_PLANE_SIZE}" \
        --node-size="${NODE_SIZE}" \
        --control-plane-count=1 \
        --networking=cilium \
        --cloud=gce \
        --project="${TF_VAR_project_id}" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodNodeSelector" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodTolerationRestriction" \
        --set="spec.metricsServer.enabled=true" \
        --set="spec.metricsServer.insecure=true" \
        --yes

    if [ -n "$spot" ]; then
        # Configure spot instances
        echo "Modifying instance groups to use spot instances..."
        kops get ig --name "${CLUSTER_NAME}" -o yaml > ig_specs.yaml
        sed -i '/spec:/a\  gcpProvisioningModel: SPOT' ig_specs.yaml
        kops replace -f ig_specs.yaml
    fi

    # Create and validate cluster
    echo "Creating the cluster..."
    kops update cluster --name="${CLUSTER_NAME}" --yes
    kops export kubeconfig --admin
    
    echo "Waiting for cluster to be ready..."
    kops validate cluster --wait 10m
    
    kubectl create namespace functions
    kubectl create namespace metrics
    # Setup node labels - get second node (first is control plane)
    #NODE_NAME=$(kubectl get nodes --no-headers -o custom-columns=":metadata.name" | tail -1)
    #kubectl label node ${NODE_NAME} workload-generator=true
    # Taint the node so only workload generator pods can run there
    #kubectl taint node ${NODE_NAME} dedicated=workload-generator:NoSchedule

    # Create the persistent volume
    kubectl apply -f manifests/pv.yaml
    # Create the persistent volume claim
    kubectl apply -f manifests/workload-generator/pvc.yaml
    # Create the deployment
    kubectl apply -f manifests/workload-generator/deployment.yaml

    # After cluster is ready, label and taint nodes
    echo "Setting up node labels and taints..."
    
    # Get node names (excluding control-plane)
    NODES=($(kubectl get nodes --no-headers -o custom-columns=":metadata.name" | tail -n +2))
    
    # First worker node for metrics
    kubectl label node ${NODES[0]} dedicated=metrics
    kubectl taint node ${NODES[0]} dedicated=metrics:NoSchedule
    
    # Second worker node for workload generator
    kubectl label node ${NODES[1]} dedicated=workload-generator
    kubectl taint node ${NODES[1]} dedicated=workload-generator:NoSchedule
    
    # Third worker node for Knative system components (serving and eventing)
    kubectl label node ${NODES[2]} dedicated=knative
    kubectl taint node ${NODES[2]} dedicated=knative:NoSchedule

    # The remaining nodes (NODES[3] and NODES[4]) are left untainted for function scaling
    echo "Remaining nodes available for function scaling"

install-knative: _check-env
    kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.16.0/serving-crds.yaml
    kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.16.0/serving-core.yaml


    kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/eventing-crds.yaml
    kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/eventing-core.yaml
    kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/in-memory-channel.yaml

    # Add NodeSelector annotation to knative namespaces
    kubectl annotate namespace knative-serving scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace knative-serving scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'
    kubectl annotate namespace knative-eventing scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace knative-eventing scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'


    # Kourier
    kubectl apply -f https://github.com/knative/net-kourier/releases/download/knative-v1.16.0/kourier.yaml

    kubectl patch configmap/config-network \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"ingress-class":"kourier.ingress.networking.knative.dev"}}'

    kubectl --namespace kourier-system get service kourier

   
    kubectl wait --for=condition=ready pod --all -n knative-serving

    kubectl patch configmap/config-domain \
      --namespace knative-serving \
      --type merge \
      --patch '{"data":{"example.com":""}}'


    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo update
    helm install prometheus prometheus-community/kube-prometheus-stack -n metrics -f manifests/prometheus/values.yaml

    # Add NodeSelector annotation to metrics namespace
    kubectl annotate namespace metrics scheduler.alpha.kubernetes.io/node-selector="dedicated=metrics"
    kubectl annotate namespace metrics scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "metrics", "effect": "NoSchedule", "key": "dedicated"}]'

    # SCRAPE INTERVALS OF KNATIVE COMPONENTS

    kubectl apply -f https://raw.githubusercontent.com/knative-extensions/monitoring/main/servicemonitor.yaml
    kubectl patch servicemonitor controller -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor autoscaler -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor activator -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor webhook -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'


    kubectl patch servicemonitor broker-filter -n knative-eventing --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor broker-ingress -n knative-eventing --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'


    # delete the pods so they are recreated with the new labels
    kubectl delete pod --all -n knative-serving
    kubectl delete pod --all -n knative-eventing
    kubectl delete pod --all -n metrics
    echo "Done"

# Destroy the cluster and clean up resources
down: _check-env
    #!/bin/bash
    set -e
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"
    kops delete cluster --name "${CLUSTER_NAME}" --yes
    terraform destroy -auto-approve

# Validate cluster status
validate: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops validate cluster

# Get cluster info
get-cluster: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops get cluster

# Export kubeconfig
get-kubeconfig: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops export kubeconfig --admin

debug-cluster: _check-env
    #!/bin/bash
    terraform apply -auto-approve
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"
    kops create cluster \
        --name="${CLUSTER_NAME}" \
        --state="${KOPS_STATE_STORE}" \
        --zones="${TF_VAR_zone}" \
        --control-plane-zones="${TF_VAR_zone}" \
        --node-count="${NODE_COUNT}" \
        --control-plane-size="${CONTROL_PLANE_SIZE}" \
        --node-size="${NODE_SIZE}" \
        --control-plane-count=1 \
        --networking=cilium \
        --cloud=gce \
        --project="${TF_VAR_project_id}" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodNodeSelector" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodTolerationRestriction" \
        --dry-run \
        --output=yaml

build:
    docker build -t luccadibenedetto/workload-generator:latest --push -f ../Dockerfile.wg ../ 
    
eventing:
    go run ../cmd/deployer/main.go --action=sequence --image=go-handler-event --name=event-handler --amount=10
    kubectl apply -f manifests/functions/sequence.yaml

    kubectl apply -f manifests/workload-generator/container-source.yaml

destroy-eventing:
    go run ../cmd/deployer/main.go --action=destroy --image=go-handler-event --name=event-handler --amount=10
    kubectl delete -f manifests/functions/sequence.yaml
    kubectl delete -f manifests/workload-generator/container-source.yaml
