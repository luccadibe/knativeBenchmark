# Required environment variables (set these before running commands)
export TF_VAR_project_id := "csb-benchmark"       # Your GCP project ID
export CLUSTER_NAME := "csb.lucca.org"            # Your cluster name
export TF_VAR_zone := "europe-west1-b"             # GCP zone for the cluster
export NODE_COUNT := "5"              # 1 monitoring + 1 workload-gen + 1 knative-system + 2 for functions
export CONTROL_PLANE_SIZE := "e2-standard-2"      # Machine type for control plane n2-standard-4n2-standard-4
export NODE_SIZE := "e2-standard-2"               # Machine type for worker nodes
export TF_VAR_bucket_name := "csb-benchmark-state"      # Must be globally unique
export TF_VAR_bucket_location := "EU"  # GCS bucket location

# Required environment variables check
_check-env:
    #!/usr/bin/env bash
    missing_vars=()
    [[ -z "${TF_VAR_project_id}" ]] && missing_vars+=("TF_VAR_project_id")
    [[ -z "${CLUSTER_NAME}" ]] && missing_vars+=("CLUSTER_NAME")
    [[ -z "${TF_VAR_zone}" ]] && missing_vars+=("TF_VAR_zone")
    [[ -z "${NODE_COUNT}" ]] && missing_vars+=("NODE_COUNT")
    [[ -z "${CONTROL_PLANE_SIZE}" ]] && missing_vars+=("CONTROL_PLANE_SIZE")
    [[ -z "${NODE_SIZE}" ]] && missing_vars+=("NODE_SIZE")
    [[ -z "${TF_VAR_bucket_name}" ]] && missing_vars+=("TF_VAR_bucket_name")
    [[ -z "${TF_VAR_bucket_location}" ]] && missing_vars+=("TF_VAR_bucket_location")
    
    if [ ${#missing_vars[@]} -ne 0 ]; then
        echo "Error: Missing required environment variables:"
        printf '%s\n' "${missing_vars[@]}"
        echo "Please set the required variables at the top of the justfile"
        exit 1
    fi

# List available recipes
default:
    @just --list

# Setup required GCP APIs
setup: _check-env
    #!/bin/bash
    gcloud services enable cloudresourcemanager.googleapis.com
    gcloud services enable compute.googleapis.com
    gcloud services enable iam.googleapis.com
    gcloud services enable container.googleapis.com
    gcloud services enable storage.googleapis.com

# Initialize terraform
init: _check-env
    terraform init

# Create and configure the cluster
up spot="": _check-env
    #!/bin/bash
    set -e
    
    # Create GCS bucket
    terraform apply -auto-approve

    # Set kOps state store
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"

    # Create cluster configuration
    echo "Creating cluster configuration..."
    kops create cluster \
        --name="${CLUSTER_NAME}" \
        --state="${KOPS_STATE_STORE}" \
        --zones="${TF_VAR_zone}" \
        --control-plane-zones="${TF_VAR_zone}" \
        --node-count="${NODE_COUNT}" \
        --control-plane-size="${CONTROL_PLANE_SIZE}" \
        --node-size="${NODE_SIZE}" \
        --control-plane-count=1 \
        --networking=cilium \
        --cloud=gce \
        --project="${TF_VAR_project_id}" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodNodeSelector" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodTolerationRestriction" \
        --set="spec.metricsServer.enabled=true" \
        --set="spec.metricsServer.insecure=true" \
        --yes

    if [ {{spot}} == "true" ]; then
        # Configure spot instances
        echo "Modifying instance groups to use spot instances..."
        kops get ig --name "${CLUSTER_NAME}" -o yaml > ig_specs.yaml
        sed -i '/spec:/a\  gcpProvisioningModel: SPOT' ig_specs.yaml
        kops replace -f ig_specs.yaml
    fi

    # Create and validate cluster
    echo "Creating the cluster..."
    kops update cluster --name="${CLUSTER_NAME}" --yes
    kops export kubeconfig --admin
    
    echo "Waiting for cluster to be ready..."
    kops validate cluster --wait 10m
    
    kubectl create namespace functions
    kubectl create namespace metrics
    kubectl create namespace workload-generator
    # Setup node labels - get second node (first is control plane)
    #NODE_NAME=$(kubectl get nodes --no-headers -o custom-columns=":metadata.name" | tail -1)
    #kubectl label node ${NODE_NAME} workload-generator=true
    # Taint the node so only workload generator pods can run there
    #kubectl taint node ${NODE_NAME} dedicated=workload-generator:NoSchedule

    # Create the persistent volume
    kubectl apply -f manifests/pv.yaml
    # Create the persistent volume claim
    kubectl apply -f manifests/workload-generator/pvc.yaml
    # Create the deployment
    kubectl apply -f manifests/workload-generator/deployment.yaml

    # After cluster is ready, label and taint nodes
    echo "Setting up node labels and taints..."
    
    # Get node names (excluding control-plane)
    NODES=($(kubectl get nodes --no-headers -o custom-columns=":metadata.name" | tail -n +2))
    
    # First worker node for metrics
    kubectl label node ${NODES[0]} dedicated=metrics
    kubectl taint node ${NODES[0]} dedicated=metrics:NoSchedule
    
    # Second worker node for workload generator
    kubectl label node ${NODES[1]} dedicated=workload-generator
    kubectl taint node ${NODES[1]} dedicated=workload-generator:NoSchedule
    
    # Third worker node for Knative system components (serving and eventing)
    kubectl label node ${NODES[2]} dedicated=knative
    kubectl taint node ${NODES[2]} dedicated=knative:NoSchedule

    # The remaining nodes (NODES[3] and NODES[4]) are left untainted for function scaling
    echo "Remaining nodes available for function scaling"

install-knative: _check-env
    kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.16.0/serving-crds.yaml
    kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.16.0/serving-core.yaml


    kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/eventing-crds.yaml
    kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/eventing-core.yaml
    #kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/in-memory-channel.yaml
    kubectl apply -f https://github.com/knative-extensions/eventing-kafka-broker/releases/download/knative-v1.16.1/eventing-kafka-controller.yaml
    kubectl apply -f https://github.com/knative-extensions/eventing-kafka-broker/releases/download/knative-v1.16.1/eventing-kafka-channel.yaml
    kubectl apply -f https://github.com/knative-extensions/eventing-kafka-broker/releases/download/knative-v1.16.1/eventing-kafka-broker.yaml

    # Add NodeSelector annotation to knative namespaces
    kubectl annotate namespace knative-serving scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace knative-serving scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'
    kubectl annotate namespace knative-eventing scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace knative-eventing scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'


    # Kourier
    kubectl apply -f https://github.com/knative/net-kourier/releases/download/knative-v1.16.0/kourier.yaml

    kubectl patch configmap/config-network \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"ingress-class":"kourier.ingress.networking.knative.dev"}}'

    kubectl --namespace kourier-system get service kourier

    # cert manager
    kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.3/cert-manager.yaml
    #
    kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml

    kubectl apply -f https://github.com/rabbitmq/messaging-topology-operator/releases/latest/download/messaging-topology-operator-with-certmanager.yaml

    kubectl wait --for=condition=ready pod --all -n knative-serving

    kubectl patch configmap/config-domain \
      --namespace knative-serving \
      --type merge \
      --patch '{"data":{"example.com":""}}'


    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo update
    helm install prometheus prometheus-community/kube-prometheus-stack -n metrics -f manifests/prometheus/values.yaml

    # Add NodeSelector annotation to metrics namespace
    kubectl annotate namespace metrics scheduler.alpha.kubernetes.io/node-selector="dedicated=metrics"
    kubectl annotate namespace metrics scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "metrics", "effect": "NoSchedule", "key": "dedicated"}]'

    # SCRAPE INTERVALS OF KNATIVE COMPONENTS

    kubectl apply -f https://raw.githubusercontent.com/knative-extensions/monitoring/main/servicemonitor.yaml
    kubectl patch servicemonitor controller -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor autoscaler -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor activator -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor webhook -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'


    kubectl patch servicemonitor broker-filter -n knative-eventing --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor broker-ingress -n knative-eventing --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'


    # delete the pods so they are recreated with the new labels
    kubectl delete pod --all -n knative-serving
    kubectl delete pod --all -n knative-eventing
    kubectl delete pod --all -n metrics
    echo "Done"

# Destroy the cluster and clean up resources

install-serving: _check-env
    kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.16.0/serving-crds.yaml
    kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.16.0/serving-core.yaml

    # Add NodeSelector annotation to knative namespaces
    kubectl annotate namespace knative-serving scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace knative-serving scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'

    kubectl wait --for=condition=ready pod --all -n knative-serving

    kubectl patch configmap/config-domain \
      --namespace knative-serving \
      --type merge \
      --patch '{"data":{"example.com":""}}'

    kubectl patch configmap/config-defaults \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"revision-memory-request":"10M"}}'

    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"min-scale-down-utilization-percentage":"100"}}'


    

    # SCRAPE INTERVALS OF KNATIVE COMPONENTS
    kubectl delete pod --all -n knative-serving
    kubectl delete pod --all -n metrics
    

install-kourier:
    # Kourier
    kubectl apply -f https://github.com/knative/net-kourier/releases/download/knative-v1.16.0/kourier.yaml

    kubectl patch configmap/config-network \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"ingress-class":"kourier.ingress.networking.knative.dev"}}'

    kubectl --namespace kourier-system get service kourier

install-eventing:
    kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/eventing-crds.yaml
    kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/eventing-core.yaml
    kubectl apply -f https://github.com/knative-extensions/eventing-rabbitmq/releases/download/knative-v1.16.0/rabbitmq-broker.yaml
    kubectl apply -f manifests/knative/logs-pv.yaml

    # You must update the StatefulSet volumeClaimTemplates to reference the shared-logs volume .https://knative.dev/docs/serving/observability/logging/collecting-logs/#setting-up-a-local-collector

    # cert manager
    kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.3/cert-manager.yaml

    # wait for cert manager to be ready
    kubectl wait --for=condition=ready pod --all -n cert-manager
    #
    kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml

    kubectl apply -f https://github.com/rabbitmq/messaging-topology-operator/releases/latest/download/messaging-topology-operator-with-certmanager.yaml
    kubectl annotate namespace knative-eventing scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace knative-eventing scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'

install-knative-metrics:
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo update
    helm install prometheus prometheus-community/kube-prometheus-stack -n metrics -f manifests/prometheus/values.yaml

    # Add NodeSelector annotation to metrics namespace
    kubectl annotate namespace metrics scheduler.alpha.kubernetes.io/node-selector="dedicated=metrics"
    kubectl annotate namespace metrics scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "metrics", "effect": "NoSchedule", "key": "dedicated"}]'

    kubectl apply -f https://raw.githubusercontent.com/knative-extensions/monitoring/main/servicemonitor.yaml
    kubectl patch servicemonitor controller -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor autoscaler -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor activator -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor webhook -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl delete pod --all -n knative-serving
    kubectl delete pod --all -n metrics

    kubectl patch servicemonitor broker-filter -n knative-eventing --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor broker-ingress -n knative-eventing --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl delete pod --all -n knative-eventing

down: _check-env
    #!/bin/bash
    set -e
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"
    kops delete cluster --name "${CLUSTER_NAME}" --yes
    terraform destroy -auto-approve

# Validate cluster status
validate: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops validate cluster

# Get cluster info
get-cluster: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops get cluster

# Export kubeconfig
get-kubeconfig: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops export kubeconfig --admin

debug-cluster: _check-env
    #!/bin/bash
    terraform apply -auto-approve
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"
    kops create cluster \
        --name="${CLUSTER_NAME}" \
        --state="${KOPS_STATE_STORE}" \
        --zones="${TF_VAR_zone}" \
        --control-plane-zones="${TF_VAR_zone}" \
        --node-count="${NODE_COUNT}" \
        --control-plane-size="${CONTROL_PLANE_SIZE}" \
        --node-size="${NODE_SIZE}" \
        --control-plane-count=1 \
        --networking=cilium \
        --cloud=gce \
        --project="${TF_VAR_project_id}" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodNodeSelector" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodTolerationRestriction" \
        --dry-run \
        --output=yaml

build:
    docker build -t luccadibenedetto/workload-generator:latest --push -f ../Dockerfile.wg ../ 
    docker build -t luccadibenedetto/cloudevent-reciever:latest --push -f ../Dockerfile.reciever ../
    
eventing:
    go run ../cmd/deployer/main.go --action=sequence --image=go-handler-event --name=event-handler --amount=10
    kubectl apply -f manifests/functions/sequence.yaml

    kubectl apply -f manifests/workload-generator/container-source.yaml

    kubectl apply -f manifests/fluent-bit/statefulset.yaml

destroy-eventing:
    go run ../cmd/deployer/main.go --action=destroy --image=go-handler-event --name=event-handler --amount=10
    kubectl delete -f manifests/functions/sequence.yaml
    kubectl delete -f manifests/workload-generator/container-source.yaml

serving-scenario-1:
    #GO
    go run ../cmd/deployer/main.go --action=deploy --image=empty-go-http --name=empty-go-http --amount=1
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-go.yaml --prefix=serving-scenario-1-go
    go run ../cmd/deployer/main.go --action=delete --image=empty-go-http --name=empty-go-http --amount=1

    #PYTHON
    go run ../cmd/deployer/main.go --action=deploy --image=empty-python-http --name=empty-python-http --amount=1
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-python.yaml --prefix=serving-scenario-1-python
    go run ../cmd/deployer/main.go --action=delete --image=empty-python-http --name=empty-python-http --amount=1

    #QUARKUS
    go run ../cmd/deployer/main.go --action=deploy --image=empty-quarkus-http --name=empty-quarkus-http --amount=1
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-quarkus.yaml --prefix=serving-scenario-1-quarkus
    go run ../cmd/deployer/main.go --action=delete --image=empty-quarkus-http --name=empty-quarkus-http --amount=1

    #SPRINGBOOT
    go run ../cmd/deployer/main.go --action=deploy --image=empty-springboot-http --name=empty-springboot-http --amount=1
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-springboot.yaml --prefix=serving-scenario-1-springboot
    go run ../cmd/deployer/main.go --action=delete --image=empty-springboot-http --name=empty-springboot-http --amount=1
    
    #RUST
    go run ../cmd/deployer/main.go --action=deploy --image=empty-rust-http --name=empty-rust-http --amount=1
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-rust.yaml --prefix=serving-scenario-1-rust
    go run ../cmd/deployer/main.go --action=delete --image=empty-rust-http --name=empty-rust-http --amount=1

    #TYPESCRIPT
    go run ../cmd/deployer/main.go --action=deploy --image=empty-ts-http --name=empty-ts-http --amount=1
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-ts.yaml --prefix=serving-scenario-1-ts
    go run ../cmd/deployer/main.go --action=delete --image=empty-ts-http --name=empty-ts-http --amount=1

    # Testing the autoscaling strategy
    go run ../cmd/deployer/main.go --action=deploy --image=empty-go-http --name=empty-go-http --amount=1
    sleep 10
    go run ../cmd/deployer/main.go --action=patch --name=empty-go-http-0 --metric=concurrency --target=20
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-go.yaml --prefix=serving-scenario-1-go-concurrency-20
    go run ../cmd/deployer/main.go --action=patch --name=empty-go-http-0 --metric=concurrency --target=40
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-go.yaml --prefix=serving-scenario-1-go-concurrency-40
    
    go run ../cmd/deployer/main.go --action=patch --name=empty-go-http-0 --metric=concurrency --target=60
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-go.yaml --prefix=serving-scenario-1-go-concurrency-60

    go run ../cmd/deployer/main.go --action=patch --name=empty-go-http-0 --metric=concurrency --target=80
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-go.yaml --prefix=serving-scenario-1-go-concurrency-80

    go run ../cmd/deployer/main.go --action=patch --name=empty-go-http-0 --metric=concurrency --target=100
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-go.yaml --prefix=serving-scenario-1-go-concurrency-100

    go run ../cmd/deployer/main.go --action=patch --name=empty-go-http-0 --metric=rps --target=50
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-go.yaml --prefix=serving-scenario-1-go-rps-50

    go run ../cmd/deployer/main.go --action=patch --name=empty-go-http-0 --metric=rps --target=100
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-go.yaml --prefix=serving-scenario-1-go-rps-100

    go run ../cmd/deployer/main.go --action=patch --name=empty-go-http-0 --metric=rps --target=150
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-go.yaml --prefix=serving-scenario-1-go-rps-150

    go run ../cmd/deployer/main.go --action=patch --name=empty-go-http-0 --metric=rps --target=200
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-1-go.yaml --prefix=serving-scenario-1-go-rps-200

serving-scenario-2:
    #!/bin/bash
    go run ../cmd/deployer/main.go --action=deploy --image=empty-go-http --name=empty-go-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-python-http --name=empty-python-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-quarkus-http --name=empty-quarkus-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-springboot-http --name=empty-springboot-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-rust-http --name=empty-rust-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-ts-http --name=empty-ts-http --amount=1

    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"scale-to-zero-grace-period":"10s"}}'
    languages=("go" "python" "quarkus" "springboot" "rust" "ts")

    # execute concurrently on the workload generator
    pod_name=$(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator)
    
    # Launch all workload generators in parallel and store PIDs
    pids=()
    for language in "${languages[@]}"; do
        kubectl exec -it $pod_name -n workload-generator -- ./workload-generator --config=serving-scenario-2-${language}.yaml --prefix=serving-scenario-2-${language} &
        pids+=($!)
    done

    # Wait for all processes to complete
    for pid in "${pids[@]}"; do
        wait $pid
    done

    # Back to the original scale-to-zero-grace-period
    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"scale-to-zero-grace-period":"30s"}}'

# Extracts the log files from the workload generator pod
logfiles:
    #All the log files are in the /logs directory
    # First zip the logs inside the pod
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- tar -czf logs.tar.gz /logs
    # Then copy the logs to the analysis directory
    kubectl cp workload-generator/$(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator):/app/logs.tar.gz ../analysis/logs/logs.tar.gz
    # Then unzip the logs
    tar -xzf ../analysis/logs/logs.tar.gz
    # Then delete the zip file
    rm ../analysis/logs/logs.tar.gz


# Extracts the csv logs from the event logger pod
extract-eventing-logs:
    #!/bin/bash
    # Get event logger pod name (match label name event-logger)
    event_logger_pod=$(kubectl get pods -l app=event-logger -o jsonpath="{.items[0].metadata.name}" -n functions)
    # Get event logger logs
    kubectl exec -n functions $event_logger_pod -- /bin/bash -c "cat /data/events.csv" > ../analysis/logs/events.csv

# Runs the eventing broker scenario
eventing-broker:
    #!/bin/bash
    kubectl apply -f manifests/rabbitmq/rabbitmq-cluster.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker-config.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker.yaml

    # wait for the pods to be ready
    sleep 70
    kubectl wait --for=condition=ready pod --all -n knative-eventing
    kubectl wait --for=condition=ready pod --all -n knative-serving

    go run ../cmd/deployer/main.go --action=deploy --image=cloudevent-reciever --name=reciever --amount=1
    #go run ../cmd/deployer/main.go --action=delete --image=cloudevent-reciever --name=reciever --amount=1

    kubectl apply -f manifests/workload-generator/container-source-1.yaml
    kubectl delete -f manifests/workload-generator/deployment.yaml


    # Define array of trigger amounts to test
    trigger_amounts=(1 10 20 30 40 50 60 70 80 90 100)

    # Run tests for each trigger amount
    for amount in "${trigger_amounts[@]}"; do
        # Deploy triggers
        go run ../cmd/deployer/main.go --action=trigger --name=reciever --broker=rabbitmq-broker --amount=$amount
        
        # Run workload generator
        kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=cloudevent.yaml --prefix=eventing-broker-${amount}  --event=true
        
        # Clean up triggers
        go run ../cmd/deployer/main.go --action=delete-trigger --name=reciever --broker=rabbitmq-broker --amount=$amount
        sleep 10
    done
run: install-serving install-kourier install-eventing eventing-broker