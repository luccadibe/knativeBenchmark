# Required environment variables (set these before running commands)
export TF_VAR_project_id := "csb-benchmark"       # Your GCP project ID
export CLUSTER_NAME := "csb.lucca.org"            # Your cluster name
export TF_VAR_zone := "europe-west1-b"             # GCP zone for the cluster
export NODE_COUNT := "5"              # 1 monitoring + 1 workload-gen + 2 knative-system + 1 for functions
export CONTROL_PLANE_SIZE := "n2-standard-4"      # Machine type for control plane n2-standard-4n2-standard-4
export NODE_SIZE := "n2-standard-4"               # Machine type for worker nodes
export TF_VAR_bucket_name := "csb-benchmark-state"      # Must be globally unique
export TF_VAR_bucket_location := "EU"  # GCS bucket location

# Required environment variables check
_check-env:
    #!/usr/bin/env bash
    missing_vars=()
    [[ -z "${TF_VAR_project_id}" ]] && missing_vars+=("TF_VAR_project_id")
    [[ -z "${CLUSTER_NAME}" ]] && missing_vars+=("CLUSTER_NAME")
    [[ -z "${TF_VAR_zone}" ]] && missing_vars+=("TF_VAR_zone")
    [[ -z "${NODE_COUNT}" ]] && missing_vars+=("NODE_COUNT")
    [[ -z "${CONTROL_PLANE_SIZE}" ]] && missing_vars+=("CONTROL_PLANE_SIZE")
    [[ -z "${NODE_SIZE}" ]] && missing_vars+=("NODE_SIZE")
    [[ -z "${TF_VAR_bucket_name}" ]] && missing_vars+=("TF_VAR_bucket_name")
    [[ -z "${TF_VAR_bucket_location}" ]] && missing_vars+=("TF_VAR_bucket_location")
    
    if [ ${#missing_vars[@]} -ne 0 ]; then
        echo "Error: Missing required environment variables:"
        printf '%s\n' "${missing_vars[@]}"
        echo "Please set the required variables at the top of the justfile"
        exit 1
    fi

# List available recipes
default:
    @just --list

# Setup required GCP APIs
setup: _check-env
    #!/bin/bash
    gcloud services enable cloudresourcemanager.googleapis.com
    gcloud services enable compute.googleapis.com
    gcloud services enable iam.googleapis.com
    gcloud services enable container.googleapis.com
    gcloud services enable storage.googleapis.com

# Initialize terraform
init: _check-env
    terraform init

# Create and configure the cluster
up spot="": _check-env
    #!/bin/bash
    set -e
    
    # Create GCS bucket
    terraform apply -auto-approve

    # Set kOps state store
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"

    # Create cluster configuration
    echo "Creating cluster configuration..."
    kops create cluster \
        --name="${CLUSTER_NAME}" \
        --state="${KOPS_STATE_STORE}" \
        --zones="${TF_VAR_zone}" \
        --control-plane-zones="${TF_VAR_zone}" \
        --node-count="${NODE_COUNT}" \
        --control-plane-size="${CONTROL_PLANE_SIZE}" \
        --node-size="${NODE_SIZE}" \
        --control-plane-count=1 \
        --networking=cilium \
        --cloud=gce \
        --project="${TF_VAR_project_id}" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodNodeSelector" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodTolerationRestriction" \
        --set="spec.metricsServer.enabled=true" \
        --set="spec.metricsServer.insecure=true" \
        --yes

    if [ ${spot} = "true" ]; then
        # Configure spot instances
        echo "Modifying instance groups to use spot instances..."
        kops get ig --name "${CLUSTER_NAME}" -o yaml > ig_specs.yaml
        sed -i '/spec:/a\  gcpProvisioningModel: SPOT' ig_specs.yaml
        kops replace -f ig_specs.yaml
    fi

    # Create and validate cluster
    echo "Creating the cluster..."
    kops update cluster --name="${CLUSTER_NAME}" --yes
    kops export kubeconfig --admin
    
    echo "Waiting for cluster to be ready..."
    kops validate cluster --wait 10m
    
    kubectl create namespace functions
    kubectl create namespace metrics
    kubectl create namespace workload-generator
    # Setup node labels - get second node (first is control plane)
    #NODE_NAME=$(kubectl get nodes --no-headers -o custom-columns=":metadata.name" | tail -1)
    #kubectl label node ${NODE_NAME} workload-generator=true
    # Taint the node so only workload generator pods can run there
    #kubectl taint node ${NODE_NAME} dedicated=workload-generator:NoSchedule

    # Create the persistent volume
    kubectl apply -f manifests/pv.yaml
    # Create the persistent volume claim
    kubectl apply -f manifests/workload-generator/pvc.yaml
    # Create the deployment
    kubectl apply -f manifests/workload-generator/deployment.yaml

    # After cluster is ready, label and taint nodes
    echo "Setting up node labels and taints..."
    
    # Get node names (excluding control-plane)
    NODES=($(kubectl get nodes --no-headers -o custom-columns=":metadata.name" | tail -n +2))
    
    # First worker node for metrics
    kubectl label node ${NODES[0]} dedicated=metrics
    kubectl taint node ${NODES[0]} dedicated=metrics:NoSchedule
    
    # Second worker node for workload generator
    kubectl label node ${NODES[1]} dedicated=workload-generator
    kubectl taint node ${NODES[1]} dedicated=workload-generator:NoSchedule
    
    # Third worker node for Knative system components (serving and eventing)
    kubectl label node ${NODES[2]} dedicated=knative
    kubectl taint node ${NODES[2]} dedicated=knative:NoSchedule

    # Fourth worker node for Knative system components (serving and eventing)
    kubectl label node ${NODES[3]} dedicated=knative
    kubectl taint node ${NODES[3]} dedicated=knative:NoSchedule

    # The remaining node (NODES[4]) is left untainted for function scaling
    echo "Remaining nodes available for function scaling"

install-serving: _check-env
    kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.16.0/serving-crds.yaml
    kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.16.0/serving-core.yaml

    # Add NodeSelector annotation to knative namespaces
    kubectl annotate namespace knative-serving scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace knative-serving scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'

    kubectl wait --for=condition=ready pod --all -n knative-serving

    kubectl patch configmap/config-domain \
      --namespace knative-serving \
      --type merge \
      --patch '{"data":{"example.com":""}}'

    kubectl patch configmap/config-defaults \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"revision-memory-request":"10M"}}'

    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"min-scale-down-utilization-percentage":"100"}}'


    

    # SCRAPE INTERVALS OF KNATIVE COMPONENTS
    kubectl delete pod --all -n knative-serving
    kubectl delete pod --all -n metrics
    

install-kourier:
    # Kourier
    kubectl apply -f https://github.com/knative/net-kourier/releases/download/knative-v1.16.0/kourier.yaml

    kubectl patch configmap/config-network \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"ingress-class":"kourier.ingress.networking.knative.dev"}}'

    kubectl --namespace kourier-system get service kourier

    kubectl annotate namespace kourier-system scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace kourier-system scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'

    kubectl delete pod --all -n kourier-system
    kubectl wait --for=condition=ready pod --all -n kourier-system

install-eventing:
    kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/eventing-crds.yaml
    kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/eventing-core.yaml
    kubectl apply -f https://github.com/knative-extensions/eventing-rabbitmq/releases/download/knative-v1.16.0/rabbitmq-broker.yaml
    kubectl apply -f manifests/knative/logs-pv.yaml

    # You must update the StatefulSet volumeClaimTemplates to reference the shared-logs volume .https://knative.dev/docs/serving/observability/logging/collecting-logs/#setting-up-a-local-collector

    # cert manager
    kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.3/cert-manager.yaml

    # wait for cert manager to be ready
    kubectl wait --for=condition=ready pod --all -n cert-manager
    #
    kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml

    kubectl apply -f https://github.com/rabbitmq/messaging-topology-operator/releases/latest/download/messaging-topology-operator-with-certmanager.yaml
    kubectl annotate namespace knative-eventing scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace knative-eventing scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'

    kubectl delete pod --all -n knative-eventing

install-knative-metrics:
    kubectl apply -f manifests/prometheus/storage-class.yaml
    kubectl apply -f manifests/prometheus/pv.yaml
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo update
    helm install prometheus prometheus-community/kube-prometheus-stack -n metrics -f manifests/prometheus/values.yaml

    # Add NodeSelector annotation to metrics namespace
    kubectl annotate namespace metrics scheduler.alpha.kubernetes.io/node-selector="dedicated=metrics"
    kubectl annotate namespace metrics scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "metrics", "effect": "NoSchedule", "key": "dedicated"}]'

    kubectl apply -f https://raw.githubusercontent.com/knative-extensions/monitoring/main/servicemonitor.yaml
    kubectl patch servicemonitor controller -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor autoscaler -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor activator -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor webhook -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl delete pod --all -n knative-serving
    kubectl delete pod --all -n metrics

    kubectl patch servicemonitor broker-filter -n knative-eventing --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor broker-ingress -n knative-eventing --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl delete pod --all -n knative-eventing

down: _check-env
    #!/bin/bash
    set -e
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"
    kops delete cluster --name "${CLUSTER_NAME}" --yes
    terraform destroy -auto-approve

# Validate cluster status
validate: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops validate cluster

# Get cluster info
get-cluster: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops get cluster

# Export kubeconfig
get-kubeconfig: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops export kubeconfig --admin

debug-cluster: _check-env
    #!/bin/bash
    terraform apply -auto-approve
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"
    kops create cluster \
        --name="${CLUSTER_NAME}" \
        --state="${KOPS_STATE_STORE}" \
        --zones="${TF_VAR_zone}" \
        --control-plane-zones="${TF_VAR_zone}" \
        --node-count="${NODE_COUNT}" \
        --control-plane-size="${CONTROL_PLANE_SIZE}" \
        --node-size="${NODE_SIZE}" \
        --control-plane-count=1 \
        --networking=cilium \
        --cloud=gce \
        --project="${TF_VAR_project_id}" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodNodeSelector" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodTolerationRestriction" \
        --dry-run \
        --output=yaml

build:
    docker build -t luccadibenedetto/workload-generator:latest --push -f ../Dockerfile.wg ../ 
    docker build -t luccadibenedetto/cloudevent-reciever:latest --push -f ../Dockerfile.reciever ../
    docker build -t luccadibenedetto/eventlogger:latest --push -f ../Dockerfile.eventlogger ../
    
eventing:
    go run ../cmd/deployer/main.go --action=sequence --image=go-handler-event --name=event-handler --amount=10
    kubectl apply -f manifests/functions/sequence.yaml

    kubectl apply -f manifests/workload-generator/container-source.yaml

    kubectl apply -f manifests/fluent-bit/statefulset.yaml

destroy-eventing:
    go run ../cmd/deployer/main.go --action=destroy --image=go-handler-event --name=event-handler --amount=10
    kubectl delete -f manifests/workload-generator/container-source-1.yaml
    kubectl delete -f manifests/rabbitmq/rabbitmq-cluster.yaml
    kubectl delete -f manifests/knative/rabbitmq-broker-config.yaml
    kubectl delete -f manifests/knative/rabbitmq-broker.yaml
    kubectl delete -f manifests/reciever/deployment.yaml

# Warm latency benchmark. Per language. RPS 1500 , 1700 , 2000, 2500
serving-scenario-1:
    #!/bin/bash
    kubectl delete -f manifests/workload-generator/container-source-1.yaml
    kubectl apply -f manifests/workload-generator/deployment.yaml
    RPS_VALUES=(5000 , 6000 , 7000 , 8000 , 9000 , 10000)
    LANGUAGES=(
        "go:empty-go-http"
        "python:empty-python-http"
        "quarkus:empty-quarkus-http"
        "springboot:empty-springboot-http"
        "rust:empty-rust-http"
        "ts:empty-ts-http"
    )
    SCENARIO="serving-scenario-1"
    NAMESPACE="workload-generator"

    # Main execution
    for rps in "${RPS_VALUES[@]}"; do
        for lang_pair in "${LANGUAGES[@]}"; do
            IFS=':' read -r lang image <<< "$lang_pair"
            
            echo "Running benchmark for $lang at $rps RPS"
            
            # Deploy the service
            go run ../cmd/deployer/main.go --action=deploy --image="$image" --name="$image" --amount=1
            sleep 10

            # Run workload generator
            kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n $NAMESPACE) -n $NAMESPACE -- \
                ./workload-generator \
                --config="$SCENARIO-$lang.yaml" \
                --rps=${rps} \
                --prefix="$SCENARIO_${rps}rps_${lang}"

            # Cleanup
            go run ../cmd/deployer/main.go --action=delete --image="$image" --name="$image" --amount=1
            sleep 20
        done
    done

#Coldstart benchmark
serving-scenario-2:
    #!/bin/bash
    go run ../cmd/deployer/main.go --action=deploy --image=empty-go-http --name=empty-go-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-python-http --name=empty-python-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-quarkus-http --name=empty-quarkus-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-springboot-http --name=empty-springboot-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-rust-http --name=empty-rust-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-ts-http --name=empty-ts-http --amount=1
    kubectl delete -f manifests/workload-generator/container-source-1.yaml
    kubectl apply -f manifests/workload-generator/deployment.yaml
    sleep 10
    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"scale-to-zero-grace-period":"10s"}}'
    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"stable-window":"10s"}}'

    pod_name=$(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator)

    sleep 10
            
    kubectl exec -it $pod_name -n workload-generator -- ./workload-generator --config=serving-scenario-2-go.yaml --prefix=serving-scenario-2_all

    sleep 10

    kubectl exec -it $pod_name -n workload-generator -- ./workload-generator --config=serving-scenario-2-go.yaml --prefix=serving-scenario-2_all

    sleep 10

    kubectl exec -it $pod_name -n workload-generator -- ./workload-generator --config=serving-scenario-2-go.yaml --prefix=serving-scenario-2_all

    sleep 10

    kubectl exec -it $pod_name -n workload-generator -- ./workload-generator --config=serving-scenario-2-go.yaml --prefix=serving-scenario-2_all

    # Back to the original scale-to-zero-grace-period               
    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"scale-to-zero-grace-period":"30s"}}'
    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"stable-window":"60s"}}'

    go run ../cmd/deployer/main.go --action=delete --image=empty-go-http --name=empty-go-http --amount=1
    go run ../cmd/deployer/main.go --action=delete --image=empty-python-http --name=empty-python-http --amount=1
    go run ../cmd/deployer/main.go --action=delete --image=empty-quarkus-http --name=empty-quarkus-http --amount=1
    go run ../cmd/deployer/main.go --action=delete --image=empty-springboot-http --name=empty-springboot-http --amount=1
    go run ../cmd/deployer/main.go --action=delete --image=empty-rust-http --name=empty-rust-http --amount=1
    go run ../cmd/deployer/main.go --action=delete --image=empty-ts-http --name=empty-ts-http --amount=1

# Container concurrency set to 1
serving-scenario-3:
    kubectl patch configmap/config-defaults \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"container-concurrency":"1"}}'
    go run ../cmd/deployer/main.go --action=delete --image=empty-go-http --name=empty-go-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-go-http --name=empty-go-http --amount=1

    kubectl apply -f manifests/workload-generator/deployment.yaml
    sleep 10
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-3-500rps-go.yaml --rps=500 --prefix=serving-scenario-3_500rps_go

    sleep 60

    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-3-100rps-go.yaml --rps=1000 --prefix=serving-scenario-3_1000rps_go

    kubectl patch configmap/config-defaults \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"container-concurrency":"0"}}'
# Extracts the log files from the workload generator pod
logfiles mode="serving":
    #!/bin/bash
    #All the log files are in the /logs directory
    # First zip the logs inside the pod
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- tar -czf logs.tar.gz /logs
    # Then copy the logs to the analysis directory
    kubectl cp workload-generator/$(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator):/app/logs.tar.gz ../analysis/logs/logs.tar.gz
    # Then unzip the logs
    tar -xzf ../analysis/logs/logs.tar.gz

    mv logs/* ../analysis/data/
    # Then delete the zip file
    rm ../analysis/logs/logs.tar.gz

    go run ../cmd/logparser/main.go --logs ../analysis/data/

    # move db to analysis/data
    timestamp=$(date +%s)
    mv benchmark.db ../analysis/data/benchmark-${timestamp}.db
    
    # combine metrics db
    #sqlite3 ../analysis/data/benchmark-${timestamp}.db "ATTACH DATABASE '../cmd/top/metrics.db' AS metrics;"
    #sqlite3 ../analysis/data/benchmark-${timestamp}.db "CREATE TABLE node_metrics as select * from metrics.node_metrics;"
    #sqlite3 ../analysis/data/benchmark-${timestamp}.db "CREATE TABLE pod_metrics as select * from metrics.pod_metrics;"
    #sqlite3 ../analysis/data/benchmark-${timestamp}.db "DETACH DATABASE metrics;"

    if [ {{mode}} == "eventing" ]; then
    event_logger_pod=$(kubectl get pods -l app=event-logger -o jsonpath="{.items[0].metadata.name}" -n functions)
    # Get event logger logs
    kubectl exec -n functions $event_logger_pod -- /bin/bash -c "cat /data/events.csv" > ../analysis/data/events-${timestamp}.csv
    uv run ../analysis/load-csv.py --db ../analysis/data/benchmark-${timestamp}.db --csv ../analysis/data/events-${timestamp}.csv
    fi

# Extracts the csv logs from the event logger pod
extract-eventing-logs:
    #!/bin/bash
    # Get event logger pod name (match label name event-logger)
    event_logger_pod=$(kubectl get pods -l app=event-logger -o jsonpath="{.items[0].metadata.name}" -n functions)
    # Get event logger logs
    timestamp=$(date +%s)
    kubectl exec -n functions $event_logger_pod -- /bin/bash -c "cat /data/events.csv" > ../analysis/data/events-${timestamp}.csv

# Runs the eventing scenario with 1 trigger.
# RPS: 100 , 200 , 300 , 400 , 450 , 500 , 600 , 700 , 800 , 900 , 1000
eventing-scenario-1:
    #!/bin/bash
    go run ../cmd/deployer/main.go --action=deploy --image=cloudevent-reciever --name=reciever --amount=1
    go run ../cmd/deployer/main.go --action=trigger --name=trigger --broker=rabbitmq-broker --amount=1
    kubectl apply -f manifests/rabbitmq/rabbitmq-cluster.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker-config.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker.yaml
    kubectl apply -f manifests/reciever/deployment.yaml
    kubectl apply -f manifests/workload-generator/container-source-1.yaml
    kubectl delete -f manifests/workload-generator/deployment.yaml
    kubectl wait --for=condition=ready pod --all -n knative-eventing
    kubectl wait --for=condition=ready pod --all -n knative-serving
    sleep 60

    
    
    rps=(1000 1500 2000 2500 3000)
    for rps in "${rps[@]}"; do
        kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=eventing-scenario-1-${rps}rps.yaml --prefix=eventing-scenario-1_${rps}rps  --event=true
        sleep 80
    done

    go run ../cmd/deployer/main.go --action=delete --image=cloudevent-reciever --name=reciever --amount=1
    go run ../cmd/deployer/main.go --action=delete-trigger --name=trigger --broker=rabbitmq-broker --amount=1

# Runs with multiple triggers ( 4 , 6 , 8 , 10) RPS: 1000, 1200, 1400, 1600, 1800, 2000 and 10 workers
eventing-scenario-2:
    #!/bin/bash
    go run ../cmd/deployer/main.go --action=deploy --image=cloudevent-reciever --name=reciever --amount=1
    go run ../cmd/deployer/main.go --action=trigger --name=trigger --broker=rabbitmq-broker --amount=1
    kubectl apply -f manifests/rabbitmq/rabbitmq-cluster.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker-config.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker.yaml
    kubectl apply -f manifests/reciever/deployment.yaml
    kubectl apply -f manifests/workload-generator/container-source-1.yaml
    kubectl delete -f manifests/workload-generator/deployment.yaml

    trigger_amounts=(4 6 8 10)
    rps=(2000 2500 3000 3500 4000 4500)
    for amount in "${trigger_amounts[@]}"; do
        for rps in "${rps[@]}"; do
        kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=eventing-scenario-1-100rps.yaml --prefix=eventing-scenario-2_${amount}triggers_${rps}rps --event=true --rps=${rps}
        sleep 100
        done
    done

    go run ../cmd/deployer/main.go --action=delete-trigger --name=trigger --broker=rabbitmq-broker --amount=1

# Runs with 1 trigger, variable workers in the trigger
eventing-scenario-3:
    #!/bin/bash
    go run ../cmd/deployer/main.go --action=deploy --image=cloudevent-reciever --name=reciever --amount=1
    kubectl apply -f manifests/rabbitmq/rabbitmq-cluster.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker-config.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker.yaml
    kubectl apply -f manifests/reciever/deployment.yaml
    kubectl apply -f manifests/workload-generator/container-source-1.yaml
    kubectl delete -f manifests/workload-generator/deployment.yaml
    go run ../cmd/deployer/main.go --action=trigger --name=trigger --broker=rabbitmq-broker --amount=1
    workers=(5 10 15 20)
    rps=(2000 2500 3000 3500 4000 4500)
    for workers in "${workers[@]}"; do
        for rps in "${rps[@]}"; do
            #  annotation "rabbitmq.eventing.knative.dev/parallelism": workers,
            go run ../cmd/deployer/main.go --action=patch-trigger --name=trigger-trigger-0 --broker=rabbitmq-broker --parallelism=$workers
            kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=eventing-scenario-1-100rps.yaml --prefix=eventing-scenario-3_${rps}rps_${workers}workers --event=true --rps=${rps}
            sleep 80
        done
    done

    go run ../cmd/deployer/main.go --action=delete-trigger --name=trigger --broker=rabbitmq-broker --amount=1

run: logfiles extract-eventing-logs
install: install-serving install-kourier install-eventing install-knative-metrics