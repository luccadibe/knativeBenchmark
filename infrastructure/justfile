# Required environment variables (set these before running commands)
export TF_VAR_project_id := "csb-benchmark"       # Your GCP project ID
export CLUSTER_NAME := "csb.lucca.org"            # Your cluster name
export TF_VAR_zone := "europe-west1-b"             # GCP zone for the cluster
export NODE_COUNT := "3"              # 1 monitoring + 1 workload-gen + 2 knative-system + 2 for functions
export CONTROL_PLANE_SIZE := "e2-standard-4"      # Machine type for control plane n2-standard-4n2-standard-4
export NODE_SIZE := "e2-standard-4"               # Machine type for worker nodes
export TF_VAR_bucket_name := "csb-benchmark-state"      # Must be globally unique
export TF_VAR_bucket_location := "EU"  # GCS bucket location

# Required environment variables check
_check-env:
    #!/usr/bin/env bash
    missing_vars=()
    [[ -z "${TF_VAR_project_id}" ]] && missing_vars+=("TF_VAR_project_id")
    [[ -z "${CLUSTER_NAME}" ]] && missing_vars+=("CLUSTER_NAME")
    [[ -z "${TF_VAR_zone}" ]] && missing_vars+=("TF_VAR_zone")
    [[ -z "${NODE_COUNT}" ]] && missing_vars+=("NODE_COUNT")
    [[ -z "${CONTROL_PLANE_SIZE}" ]] && missing_vars+=("CONTROL_PLANE_SIZE")
    [[ -z "${NODE_SIZE}" ]] && missing_vars+=("NODE_SIZE")
    [[ -z "${TF_VAR_bucket_name}" ]] && missing_vars+=("TF_VAR_bucket_name")
    [[ -z "${TF_VAR_bucket_location}" ]] && missing_vars+=("TF_VAR_bucket_location")
    
    if [ ${#missing_vars[@]} -ne 0 ]; then
        echo "Error: Missing required environment variables:"
        printf '%s\n' "${missing_vars[@]}"
        echo "Please set the required variables at the top of the justfile"
        exit 1
    fi

# List available recipes
default:
    @just --list

# Setup required GCP APIs
setup: _check-env
    #!/bin/bash
    gcloud services enable cloudresourcemanager.googleapis.com
    gcloud services enable compute.googleapis.com
    gcloud services enable iam.googleapis.com
    gcloud services enable container.googleapis.com
    gcloud services enable storage.googleapis.com

# Initialize terraform
init: _check-env
    terraform init

# Create and configure the cluster
up spot="": _check-env
    #!/bin/bash
    set -e
    
    # Create GCS bucket
    terraform apply -auto-approve

    # Set kOps state store
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"

    # Create cluster configuration
    echo "Creating cluster configuration..."
    kops create cluster \
        --name="${CLUSTER_NAME}" \
        --state="${KOPS_STATE_STORE}" \
        --zones="${TF_VAR_zone}" \
        --control-plane-zones="${TF_VAR_zone}" \
        --node-count="${NODE_COUNT}" \
        --control-plane-size="${CONTROL_PLANE_SIZE}" \
        --node-size="${NODE_SIZE}" \
        --control-plane-count=1 \
        --networking=cilium \
        --cloud=gce \
        --project="${TF_VAR_project_id}" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodNodeSelector" \
        --set="spec.kubeAPIServer.enableAdmissionPlugins=PodTolerationRestriction" \
        --set="spec.metricsServer.enabled=true" \
        --set="spec.metricsServer.insecure=true" \
        --yes

setup-cluster:
    #!/bin/bash
    set -e
    # Create and validate cluster
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"
    echo "Creating the cluster..."
    kops update cluster --name="${CLUSTER_NAME}" --yes
    kops export kubeconfig --admin
    
    echo "Waiting for cluster to be ready..."
    kops validate cluster --wait 10m
    
    kubectl create namespace functions
    kubectl create namespace metrics
    kubectl create namespace workload-generator

    # Create the persistent volume
    kubectl apply -f manifests/pv.yaml
    # Create the persistent volume claim
    kubectl apply -f manifests/workload-generator/pvc.yaml
    # Create the deployment
    kubectl apply -f manifests/workload-generator/deployment.yaml

    # After cluster is ready, label and taint nodes
    echo "Setting up node labels and taints..."
    
    # Get node names (excluding control-plane)
    NODES=($(kubectl get nodes --no-headers -o custom-columns=":metadata.name" | tail -n +2))
    
    # First worker node for metrics
    kubectl label node ${NODES[0]} dedicated=metrics
    #kubectl taint node ${NODES[0]} dedicated=metrics:NoSchedule
    
    # Second worker node for workload generator
    kubectl label node ${NODES[1]} dedicated=workload-generator
    kubectl taint node ${NODES[1]} dedicated=workload-generator:NoSchedule
    
    # Third worker node for Knative system components (serving and eventing)
    kubectl label node ${NODES[2]} dedicated=knative
    kubectl taint node ${NODES[2]} dedicated=knative:NoSchedule

    # Fourth worker node for Knative system components (serving and eventing)
    #kubectl label node ${NODES[3]} dedicated=knative
    #kubectl taint node ${NODES[3]} dedicated=knative:NoSchedule

    # The remaining node (NODES[4]) is left untainted for function scaling
    echo "Remaining nodes available for function scaling"

install-serving: _check-env
    kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.16.0/serving-crds.yaml
    kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.16.0/serving-core.yaml

    # Add NodeSelector annotation to knative namespaces
    kubectl annotate namespace knative-serving scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace knative-serving scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'

    kubectl wait --for=condition=ready pod --all -n knative-serving

    kubectl patch configmap/config-domain \
      --namespace knative-serving \
      --type merge \
      --patch '{"data":{"example.com":""}}'

    kubectl patch configmap/config-defaults \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"revision-memory-request":"10M"}}'

    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"min-scale-down-utilization-percentage":"100"}}'

    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"max-scale":"100"}}'
    

    # SCRAPE INTERVALS OF KNATIVE COMPONENTS
    kubectl delete pod --all -n knative-serving
    kubectl delete pod --all -n metrics
    

install-kourier:
    # Kourier
    kubectl apply -f https://github.com/knative/net-kourier/releases/download/knative-v1.16.0/kourier.yaml

    kubectl patch configmap/config-network \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"ingress-class":"kourier.ingress.networking.knative.dev"}}'

    kubectl --namespace kourier-system get service kourier

    kubectl annotate namespace kourier-system scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace kourier-system scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'

    kubectl delete pod --all -n kourier-system
    kubectl wait --for=condition=ready pod --all -n kourier-system

install-eventing:
    kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/eventing-crds.yaml
    kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.16.3/eventing-core.yaml
    kubectl apply -f https://github.com/knative-extensions/eventing-rabbitmq/releases/download/knative-v1.16.0/rabbitmq-broker.yaml
    kubectl apply -f manifests/knative/logs-pv.yaml

    # You must update the StatefulSet volumeClaimTemplates to reference the shared-logs volume .https://knative.dev/docs/serving/observability/logging/collecting-logs/#setting-up-a-local-collector

    # cert manager
    kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.3/cert-manager.yaml

    # wait for cert manager to be ready
    kubectl wait --for=condition=ready pod --all -n cert-manager
    #
    kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml

    kubectl apply -f https://github.com/rabbitmq/messaging-topology-operator/releases/latest/download/messaging-topology-operator-with-certmanager.yaml
    kubectl annotate namespace knative-eventing scheduler.alpha.kubernetes.io/node-selector="dedicated=knative"
    kubectl annotate namespace knative-eventing scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "knative", "effect": "NoSchedule", "key": "dedicated"}]'

    kubectl delete pod --all -n knative-eventing

install-knative-metrics:
    kubectl apply -f manifests/prometheus/storage-class.yaml
    kubectl apply -f manifests/prometheus/pv.yaml
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo update
    helm install prometheus prometheus-community/kube-prometheus-stack -n metrics -f manifests/prometheus/values.yaml

    # Add NodeSelector annotation to metrics namespace
    kubectl annotate namespace metrics scheduler.alpha.kubernetes.io/node-selector="dedicated=metrics"
    kubectl annotate namespace metrics scheduler.alpha.kubernetes.io/defaultTolerations='[{"operator": "Equal", "value": "metrics", "effect": "NoSchedule", "key": "dedicated"}]'

    kubectl apply -f https://raw.githubusercontent.com/knative-extensions/monitoring/main/servicemonitor.yaml
    kubectl patch servicemonitor controller -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor autoscaler -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor activator -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor webhook -n knative-serving --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl delete pod --all -n knative-serving
    kubectl delete pod --all -n metrics

    kubectl patch servicemonitor broker-filter -n knative-eventing --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl patch servicemonitor broker-ingress -n knative-eventing --type json -p '[{"op": "replace", "path": "/spec/endpoints/0/interval", "value": "5s"}]'
    kubectl delete pod --all -n knative-eventing

down: _check-env
    #!/bin/bash
    set -e
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)"
    kops delete cluster --name "${CLUSTER_NAME}" --yes
    terraform destroy -auto-approve

# Validate cluster status
validate: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops validate cluster

# Get cluster info
get-cluster: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops get cluster

# Export kubeconfig
get-kubeconfig: _check-env
    export KOPS_STATE_STORE="gs://$(terraform output -raw kops_state_store_bucket_name)" && \
    kops export kubeconfig --admin

build:
    docker build -t luccadibenedetto/workload-generator:latest --push -f ../Dockerfile.wg ../ 
    docker build -t luccadibenedetto/cloudevent-reciever:latest --push -f ../Dockerfile.reciever ../
    docker build -t luccadibenedetto/eventlogger:latest --push -f ../Dockerfile.eventlogger ../
    
eventing:
    go run ../cmd/deployer/main.go --action=sequence --image=go-handler-event --name=event-handler --amount=10
    kubectl apply -f manifests/functions/sequence.yaml

    kubectl apply -f manifests/workload-generator/container-source.yaml

    kubectl apply -f manifests/fluent-bit/statefulset.yaml

deploy-ksvc:
    go run ../cmd/deployer/main.go --action=deploy --image=empty-go-http --name=empty-go-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-python-http --name=empty-python-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-quarkus-http --name=empty-quarkus-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-springboot-http --name=empty-springboot-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-rust-http --name=empty-rust-http --amount=1
    go run ../cmd/deployer/main.go --action=deploy --image=empty-ts-http --name=empty-ts-http --amount=1
    sleep 120
# Warm latency benchmark. Per language. RPS 1500 , 1700 , 2000, 2500
serving-scenario-1:
    #!/bin/bash
    kubectl delete -f manifests/workload-generator/container-source-1.yaml
    kubectl apply -f manifests/workload-generator/deployment.yaml
    #"springboot:empty-springboot-http"
    #"python:empty-python-http"
    #"quarkus:empty-quarkus-http"
    RPS_VALUES=(16000)
    LANGUAGES=(
        "go:empty-go-http"
        "rust:empty-rust-http"
        "ts:empty-ts-http"
    )

    # Main execution
    for rps in "${RPS_VALUES[@]}"; do
        for lang_pair in "${LANGUAGES[@]}"; do
            IFS=':' read -r lang image <<< "$lang_pair"
            
            echo "Running benchmark for $lang at $rps RPS"

            # Run workload generator
            kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- \
                ./workload-generator \
                --config="serving-scenario-1-$lang.yaml" \
                --rps=${rps} \
                --prefix="serving-scenario-1_${rps}rps_${lang}"
            echo "Waiting for 180 seconds"
            sleep 180 
        done
    done

#Coldstart benchmark
serving-scenario-2:
    #!/bin/bash
    kubectl delete -f manifests/workload-generator/container-source-1.yaml
    kubectl apply -f manifests/workload-generator/deployment.yaml
    sleep 10
    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"scale-to-zero-grace-period":"10s"}}'
    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"stable-window":"10s"}}'

    pod_name=$(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator)

    sleep 10

    echo "Running cold start benchmark (40m)"
            
    for i in {1..7}; do
        kubectl exec -it $pod_name -n workload-generator -- ./workload-generator --config=serving-scenario-2-go-long.yaml --prefix=serving-scenario-2_all --cold-start=true
        sleep 10
    done

    # Back to the original scale-to-zero-grace-period               
    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"scale-to-zero-grace-period":"30s"}}'
    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"stable-window":"60s"}}'

    sleep 120

# Container concurrency set to 1
serving-scenario-3:
    #!/bin/bash

    go run ../cmd/deployer/main.go --action=deploy --image=sleep-go-http --name=sleep-go-http --amount=1

    kubectl patch configmap/config-autoscaler \
        --namespace knative-serving \
        --type merge \
        --patch '{"data":{"max-scale":"400"}}'
    
    # Patch the container concurrency to 1
    kubectl patch ksvc sleep-go-http-0 --namespace functions --type merge --patch '{"spec":{"template":{"spec":{"containerConcurrency":1}}}}'

    sleep 60
    kubectl apply -f manifests/workload-generator/deployment.yaml
    rps=(300 400)
    for rps in "${rps[@]}"; do
        kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=serving-scenario-3-100rps-go.yaml --rps=${rps} --prefix=serving-scenario-3_${rps}rps_go
        echo "Waiting for 120 seconds"
        sleep 120
    done

    # Patch it back to 0
    kubectl patch ksvc empty-go-http-0 --namespace functions --type merge --patch '{"spec":{"template":{"spec":{"containerConcurrency":0}}}}'
    sleep 120
   
process-logs:
    #!/bin/bash
    go run ../cmd/logparser/main.go --logs ../data/
    mv benchmark.db ../data/benchmark.db

    mv metrics.db ../data/metrics.db

    # Combine metrics db
    #sqlite3 ../data/benchmark.db "ATTACH DATABASE 'metrics.db' AS metrics;"
    #sqlite3 ../data/benchmark.db "CREATE TABLE node_metrics as select * from metrics.node_metrics;"
    #sqlite3 ../data/benchmark.db "CREATE TABLE pod_metrics as select * from metrics.pod_metrics;"
    #sqlite3 ../data/benchmark.db "DETACH DATABASE metrics;"

    echo "Processing logs done. The results are in ../data/benchmark.db and ../data/metrics.db"

extract-serving-logs:
    #!/bin/bash
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- tar -czf logs.tar.gz /logs
    kubectl cp workload-generator/$(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator):/app/logs.tar.gz ../data/logs.tar.gz
    tar -xzf ../data/logs.tar.gz -C ../data/
    mv ../data/logs/* ../data/
    rm ../data/logs.tar.gz

extract-logs-remove:
    #!/bin/bash
    # Extracts the logs from the workload generator pod
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- tar -czf logs.tar.gz /logs
    # Then copy the logs to the analysis directory
    kubectl cp workload-generator/$(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator):/app/logs.tar.gz ../analysis/logs/logs.tar.gz
    # rename and move the logs to the analysis directory
    timestamp=$(date +%s)
    mv ../analysis/logs/logs.tar.gz ../analysis/logs/logs-${timestamp}.tar.gz
    # remove the logs from the pod
    kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- sh -c "rm /logs/*.log"

# Extracts the csv logs from the event logger pod
extract-eventing-logs:
    #!/bin/bash
    # Get event logger pod name (match label name event-logger)
    event_logger_pod=$(kubectl get pods -l app=event-logger -o jsonpath="{.items[0].metadata.name}" -n functions)
    # Get event logger logs
    if [ ! -f ../data/events.csv ]; then
        kubectl exec -n functions $event_logger_pod -- /bin/bash -c "cat /data/events.csv" > ../data/events.csv
    else
        echo "Events file already exists. please rename it. You don't want to lose the old one"
    fi


# Runs the eventing scenario with 1 trigger.
eventing-scenario-1:
    #!/bin/bash
    go run ../cmd/deployer/main.go --action=deploy --image=cloudevent-reciever --name=reciever --amount=1
    go run ../cmd/deployer/main.go --action=trigger --name=trigger --broker=rabbitmq-broker --amount=1
    kubectl apply -f manifests/rabbitmq/rabbitmq-cluster.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker-config.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker.yaml
    kubectl apply -f manifests/reciever/deployment.yaml
    kubectl apply -f manifests/workload-generator/container-source-1.yaml
    kubectl delete -f manifests/workload-generator/deployment.yaml
    kubectl wait --for=condition=ready pod --all -n knative-eventing
    kubectl wait --for=condition=ready pod --all -n knative-serving
    echo "Waiting for 120 seconds"
    sleep 120
    
    rps=(10000 15000)
    for rps in "${rps[@]}"; do
        kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=eventing-scenario-1-100rps.yaml --prefix=eventing-scenario-1_${rps}rps  --event=true --rps=${rps}
        echo "Waiting for 60 seconds"
        sleep 60
    done
    go run ../cmd/deployer/main.go --action=delete-trigger --name=trigger --broker=rabbitmq-broker --amount=1

# Runs with multiple triggers
eventing-scenario-2:
    #!/bin/bash
    kubectl apply -f manifests/rabbitmq/rabbitmq-cluster.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker-config.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker.yaml
    kubectl apply -f manifests/reciever/deployment.yaml
    kubectl apply -f manifests/workload-generator/container-source-1.yaml
    kubectl delete -f manifests/workload-generator/deployment.yaml

    trigger_amounts=(4 6 8 10)
    rps=(3000 4000)
    for amount in "${trigger_amounts[@]}"; do
        echo "Running eventing scenario 2 with ${amount} triggers"
        go run ../cmd/deployer/main.go --action=trigger --name=trigger --broker=rabbitmq-broker --amount=${amount}
        sleep 25
        for rps in "${rps[@]}"; do
        kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=eventing-scenario-1-100rps.yaml --prefix=eventing-scenario-2_${amount}triggers_${rps}rps --event=true --rps=${rps}
        echo "Waiting for 60 seconds"
        sleep 60
        done
        go run ../cmd/deployer/main.go --action=delete-trigger --name=trigger --broker=rabbitmq-broker --amount=${amount}
    done

    

# Runs with 1 trigger, variable workers in the trigger
eventing-scenario-3:
    #!/bin/bash
    kubectl apply -f manifests/rabbitmq/rabbitmq-cluster.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker-config.yaml
    kubectl apply -f manifests/knative/rabbitmq-broker.yaml
    kubectl apply -f manifests/reciever/deployment.yaml
    kubectl apply -f manifests/workload-generator/container-source-1.yaml
    kubectl delete -f manifests/workload-generator/deployment.yaml
    go run ../cmd/deployer/main.go --action=trigger --name=trigger --broker=rabbitmq-broker --amount=1
    workers=(5 15 20)
    rps=(10000 15000)
    for workers in "${workers[@]}"; do
        for rps in "${rps[@]}"; do
            #  annotation "rabbitmq.eventing.knative.dev/parallelism": workers,
            go run ../cmd/deployer/main.go --action=patch-trigger --name=trigger-trigger-0 --broker=rabbitmq-broker --parallelism=$workers
            kubectl exec -it $(kubectl get pods -o jsonpath="{.items[0].metadata.name}" -n workload-generator) -n workload-generator -- ./workload-generator --config=eventing-scenario-1-100rps.yaml --prefix=eventing-scenario-3_${rps}rps_${workers}workers --event=true --rps=${rps}
            echo "Waiting for 80 seconds"
            sleep 80
        done
    done

    go run ../cmd/deployer/main.go --action=delete-trigger --name=trigger --broker=rabbitmq-broker --amount=1

install: install-serving install-kourier install-eventing install-knative-metrics

top:
    go run ../cmd/top/main.go --frequency=1 --storage=sqlite


runserving: deploy-ksvc serving-scenario-1 serving-scenario-2 serving-scenario-3 extract-serving-logs process-logs